
@article{doshi_chatgpt_2023,
	title = {{ChatGPT}: {Temptations} of {Progress}},
	volume = {23},
	issn = {1526-5161},
	shorttitle = {{ChatGPT}},
	url = {https://doi.org/10.1080/15265161.2023.2180110},
	doi = {10.1080/15265161.2023.2180110},
	number = {4},
	urldate = {2023-04-20},
	journal = {The American Journal of Bioethics},
	author = {Doshi, Rushabh H. and Bajaj, Simar S. and Krumholz, Harlan M.},
	month = apr,
	year = {2023},
	pmid = {36853242},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/15265161.2023.2180110},
	pages = {6--8},
	file = {Full Text PDF:C\:\\Users\\roros\\Zotero\\storage\\CLBSF7BX\\Doshi et al. - 2023 - ChatGPT Temptations of Progress.pdf:application/pdf},
}

@article{open_science_collaboration_estimating_2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.aac4716},
	doi = {10.1126/science.aac4716},
	abstract = {Empirically analyzing empirical evidence
            
              One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts
              et al.
              describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.
            
            
              Science
              , this issue
              10.1126/science.aac4716
            
          , 
            A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.
          , 
            
              INTRODUCTION
              Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.
            
            
              RATIONALE
              There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.
            
            
              RESULTS
              
                We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and
                P
                values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (
                M
                r
                = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (
                M
                r
                = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (
                P
                {\textless} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
              
            
            
              CONCLUSION
              
                No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original
                P
                value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.
              
              Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.
              
                
                  Original study effect size versus replication effect size (correlation coefficients).
                  Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.
                
                
              
            
          , 
            Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	language = {en},
	number = {6251},
	urldate = {2023-04-20},
	journal = {Science},
	author = {{Open Science Collaboration}},
	month = aug,
	year = {2015},
	pages = {aac4716},
	file = {Open Science Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf:C\:\\Users\\roros\\Zotero\\storage\\D4LVLLB2\\Open Science Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf:application/pdf},
}

@article{sallam_chatgpt_2023,
	title = {{ChatGPT} {Utility} in {Healthcare} {Education}, {Research}, and {Practice}: {Systematic} {Review} on the {Promising} {Perspectives} and {Valid} {Concerns}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-9032},
	shorttitle = {{ChatGPT} {Utility} in {Healthcare} {Education}, {Research}, and {Practice}},
	url = {https://www.mdpi.com/2227-9032/11/6/887},
	doi = {10.3390/healthcare11060887},
	abstract = {ChatGPT is an artificial intelligence (AI)-based conversational large language model (LLM). The potential applications of LLMs in health care education, research, and practice could be promising if the associated valid concerns are proactively examined and addressed. The current systematic review aimed to investigate the utility of ChatGPT in health care education, research, and practice and to highlight its potential limitations. Using the PRIMSA guidelines, a systematic search was conducted to retrieve English records in PubMed/MEDLINE and Google Scholar (published research or preprints) that examined ChatGPT in the context of health care education, research, or practice. A total of 60 records were eligible for inclusion. Benefits of ChatGPT were cited in 51/60 (85.0\%) records and included: (1) improved scientific writing and enhancing research equity and versatility; (2) utility in health care research (efficient analysis of datasets, code generation, literature reviews, saving time to focus on experimental design, and drug discovery and development); (3) benefits in health care practice (streamlining the workflow, cost saving, documentation, personalized medicine, and improved health literacy); and (4) benefits in health care education including improved personalized learning and the focus on critical thinking and problem-based learning. Concerns regarding ChatGPT use were stated in 58/60 (96.7\%) records including ethical, copyright, transparency, and legal issues, the risk of bias, plagiarism, lack of originality, inaccurate content with risk of hallucination, limited knowledge, incorrect citations, cybersecurity issues, and risk of infodemics. The promising applications of ChatGPT can induce paradigm shifts in health care education, research, and practice. However, the embrace of this AI chatbot should be conducted with extreme caution considering its potential limitations. As it currently stands, ChatGPT does not qualify to be listed as an author in scientific articles unless the ICMJE/COPE guidelines are revised or amended. An initiative involving all stakeholders in health care education, research, and practice is urgently needed. This will help to set a code of ethics to guide the responsible use of ChatGPT among other LLMs in health care and academia.},
	language = {en},
	number = {6},
	urldate = {2023-04-20},
	journal = {Healthcare},
	author = {Sallam, Malik},
	month = jan,
	year = {2023},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {artificial intelligence, digital health, ethics, healthcare, machine learning},
	pages = {887},
	file = {Full Text PDF:C\:\\Users\\roros\\Zotero\\storage\\J8SRFLRW\\Sallam - 2023 - ChatGPT Utility in Healthcare Education, Research,.pdf:application/pdf},
}

@misc{noauthor_ai_nodate,
	title = {The {AI} writing on the wall {\textbar} {Nature} {Machine} {Intelligence}},
	url = {https://www.nature.com/articles/s42256-023-00613-9#citeas},
	urldate = {2023-04-20},
	file = {The AI writing on the wall | Nature Machine Intelligence:C\:\\Users\\roros\\Zotero\\storage\\ZSS6BRAF\\s42256-023-00613-9.html:text/html},
}

